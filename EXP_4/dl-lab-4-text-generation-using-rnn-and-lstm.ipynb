{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10689716,"sourceType":"datasetVersion","datasetId":6623322}],"dockerImageVersionId":30887,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport csv\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:46:58.040645Z","iopub.execute_input":"2025-03-26T09:46:58.040902Z","iopub.status.idle":"2025-03-26T09:47:01.517792Z","shell.execute_reply.started":"2025-03-26T09:46:58.040881Z","shell.execute_reply":"2025-03-26T09:47:01.517156Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the Dataset\ntext = \"\"\nwith open(\"/kaggle/input/poems-dataset/poems-100.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        text += \" \".join(row) + \" \"                          ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.518577Z","iopub.execute_input":"2025-03-26T09:47:01.518989Z","iopub.status.idle":"2025-03-26T09:47:01.532313Z","shell.execute_reply.started":"2025-03-26T09:47:01.518959Z","shell.execute_reply":"2025-03-26T09:47:01.530633Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5a9464c2a95b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/poems-dataset/poems-100.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/poems-dataset/poems-100.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/poems-dataset/poems-100.csv'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Tokenize the Text into Words\ntokens = text.split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.532770Z","iopub.status.idle":"2025-03-26T09:47:01.533118Z","shell.execute_reply":"2025-03-26T09:47:01.532944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Dictionary to Map Words to Indices\nword_to_idx = {}\nidx_to_word = {}\nvocab_size = 0\n\nfor word in tokens:\n    if word not in word_to_idx:\n        word_to_idx[word] = vocab_size\n        idx_to_word[vocab_size] = word\n        vocab_size += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.533803Z","iopub.status.idle":"2025-03-26T09:47:01.534189Z","shell.execute_reply":"2025-03-26T09:47:01.534006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Tokens to Indices\ntoken_indices = [word_to_idx[word] for word in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.535019Z","iopub.status.idle":"2025-03-26T09:47:01.535410Z","shell.execute_reply":"2025-03-26T09:47:01.535238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Vocabulary Size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.536373Z","iopub.status.idle":"2025-03-26T09:47:01.536744Z","shell.execute_reply":"2025-03-26T09:47:01.536576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Sequences and Targets\nseq_length = 10\nsequences = []\ntargets = []\n\nfor i in range(len(token_indices) - seq_length):\n    seq = token_indices[i:i + seq_length]\n    target = token_indices[i + seq_length]\n    sequences.append(seq)\n    targets.append(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.537675Z","iopub.status.idle":"2025-03-26T09:47:01.538034Z","shell.execute_reply":"2025-03-26T09:47:01.537878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to PyTorch Tensors\nsequences = torch.tensor(sequences, dtype = torch.long)\ntargets = torch.tensor(targets, dtype = torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.538758Z","iopub.status.idle":"2025-03-26T09:47:01.539133Z","shell.execute_reply":"2025-03-26T09:47:01.538954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define One-Hot Encoding for RNN Model\nclass OneHotRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_dim, output_dim):\n        super(OneHotRNN, self).__init__()\n        self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first = True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        output, _ = self.rnn(x)\n        out = self.fc(output[:, -1, :])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.540409Z","iopub.status.idle":"2025-03-26T09:47:01.540779Z","shell.execute_reply":"2025-03-26T09:47:01.540617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define LSTM Model with Embedding Layer\nclass PoemLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(PoemLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.lstm(embedded)\n        out = self.fc(output[:, -1, :])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.541436Z","iopub.status.idle":"2025-03-26T09:47:01.541803Z","shell.execute_reply":"2025-03-26T09:47:01.541646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nembed_dim = 100\nhidden_dim = 128\noutput_dim = vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.542722Z","iopub.status.idle":"2025-03-26T09:47:01.543035Z","shell.execute_reply":"2025-03-26T09:47:01.542921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Models\nonehot_model = OneHotRNN(vocab_size, hidden_dim, output_dim)\nembedding_model = PoemLSTM(vocab_size, embed_dim, hidden_dim, output_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.544198Z","iopub.status.idle":"2025-03-26T09:47:01.544521Z","shell.execute_reply":"2025-03-26T09:47:01.544360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nonehot_optimizer = optim.Adam(onehot_model.parameters(), lr = 0.001)\nembedding_optimizer = optim.Adam(embedding_model.parameters(), lr = 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.545199Z","iopub.status.idle":"2025-03-26T09:47:01.545475Z","shell.execute_reply":"2025-03-26T09:47:01.545363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss Tracking\nonehot_losses, embedding_losses = [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.546319Z","iopub.status.idle":"2025-03-26T09:47:01.546636Z","shell.execute_reply":"2025-03-26T09:47:01.546528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Function with Tracking\ndef train_model(model, optimizer, name):\n    start_time = time.time()\n    for epoch in range(100):\n        total_loss = 0\n        for i in range(0, len(sequences), 32):\n            batch_seq = sequences[i:i + 32]\n            batch_target = targets[i:i + 32]\n\n            # One-Hot Encoding for OneHotRNN\n            if name == \"OneHotRNN\":\n                batch_seq = F.one_hot(batch_seq, num_classes = vocab_size).float()\n\n            # Forward Pass\n            outputs = model(batch_seq)\n            loss = criterion(outputs, batch_target)\n\n            # Backward Pass and Optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / (len(sequences) // 32)\n        if name == \"OneHotRNN\":\n            onehot_losses.append(avg_loss)\n        else:\n            embedding_losses.append(avg_loss)\n\n        print(f\"{name} Epoch [{epoch+1}/100], Avg Loss: {avg_loss:.4f}\")\n    print(f\"{name} Training Time: {time.time() - start_time:.2f}s\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.547218Z","iopub.status.idle":"2025-03-26T09:47:01.547505Z","shell.execute_reply":"2025-03-26T09:47:01.547388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Poem Generation Function\ndef generate_poem(model, seed_text, num_words = 50, model_type = \"EmbeddingLSTM\"):\n    model.eval()\n    words = seed_text.split()\n    with torch.no_grad():\n        for _ in range(num_words):\n            seq = [word_to_idx.get(word, 0) for word in words[-seq_length:]]\n            seq = torch.tensor(seq, dtype = torch.long).unsqueeze(0)\n\n            if model_type == \"OneHotRNN\":\n                seq = F.one_hot(seq, num_classes = vocab_size).float()\n\n            output = model(seq)\n            probabilities = F.softmax(output, dim = 1)\n            predicted_idx = torch.multinomial(probabilities, 1).item()\n\n            words.append(idx_to_word[predicted_idx])\n\n    return \" \".join(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.548036Z","iopub.status.idle":"2025-03-26T09:47:01.548339Z","shell.execute_reply":"2025-03-26T09:47:01.548237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train Models\ntrain_model(onehot_model, onehot_optimizer, \"OneHotRNN\")\ntrain_model(embedding_model, embedding_optimizer, \"EmbeddingLSTM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.548882Z","iopub.status.idle":"2025-03-26T09:47:01.549148Z","shell.execute_reply":"2025-03-26T09:47:01.549014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Poems\nseed_text = \"I wandered lonely as a\"\nprint(\"\\nGenerated Poem (OneHotRNN):\", generate_poem(onehot_model, seed_text, model_type = \"OneHotRNN\"))\nprint(\"\\nGenerated Poem (EmbeddingLSTM):\", generate_poem(embedding_model, seed_text, model_type = \"EmbeddingLSTM\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:47:01.549745Z","iopub.status.idle":"2025-03-26T09:47:01.549971Z","shell.execute_reply":"2025-03-26T09:47:01.549880Z"}},"outputs":[],"execution_count":null}]}